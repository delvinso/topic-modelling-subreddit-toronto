library(tidyverse)
library(wordclouds)

# ---- read in our LDA models ---- 
toronto_train_lda20 <- readRDS("toronto_train_lda_20_topics_2016_to_2018_09.RDS")
toronto_train_lda50 <- readRDS("toronto_train_lda_50_topics_2016_to_2018_09.RDS")

# ---- visualizing LDA results ---- 


# toronto_topics <- broom::tidy(toronto_lda20 , matrix = "beta")
toronto_topics <- broom::tidy(toronto_train_lda50, matrix = "beta")
# toronto_topics <- broom::tidy(toronto_train_lda20, matrix = "beta")

# take the top terms by topic, and probability 
toronto_top_topics <- toronto_topics %>% 
  group_by(topic) %>% 
  top_n(30, beta)  %>%
  arrange(topic, beta)

toronto_top_topics

# compute total word counts for labelling on graph
toronto_all_word_count <- toronto_clean_words_count %>% 
  group_by(word) %>% 
  summarize(total_n = sum(n))

top_topic_terms <- toronto_top_topics %>%
  # so we can visualize overall frequency
  left_join(toronto_all_word_count, by = c("term" = "word")) %>%
  ungroup() %>%
  mutate(row = (row_number()))  %>%
  mutate(term = gsub("wwwtorontopubliclibraryca", "wwwtoronto\npublic\nlibraryca", term))
  


# ---- bar graph of overall term frequency, ordered by beta ----
# png(filename = "output/toronto_topic_modeling_10.png", width = 1080, height = 720)
# png(filename = "output/toronto_topic_modeling_20_train.png", width = 1400, height = 900)
png(filename = "output/toronto_topic_modeling_50_train.png", width = 1400, height = 900)

# CHANGE TITLE IF USING TRAIN OR FULL DATA
top_topic_terms %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ggplot() + 
  geom_col(aes(x = row, y = beta, fill = topic ), show.legend = FALSE) + 
  geom_label(aes(x = row, y = beta, label = scales::comma(total_n), fill = topic), 
             hjust = -0.1, show.legend = FALSE, size = 3) + 
  facet_wrap(~ topic, scales = "free", nrow = 5) + 
  scale_fill_distiller(palette = "Dark2") +
  scale_x_continuous(
    breaks = top_topic_terms$row,
    labels = top_topic_terms$term,
    expand = c(0,0)
  ) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 3))) + # 0.5 for 20,  3 for 50
  coord_flip() +
  hrbrthemes::theme_ipsum_rc(base_size = 12, grid = "X") + # size 16 for 20, size 12 for 50
  theme(axis.text.x = element_blank()) +
  ggtitle("Topics Generated from /r/Toronto by LDA",
          "1,536,986 comments made over 53, 841 posts from 2016 - 2018 September. \nOverall term frequency is shown, but words are sorted by probability of being generated by a given topic.") +
  labs(y = "Probability of Word Being Generated by Topic", x = "", caption = "delvinso.github.io")
dev.off()

# ---- bar graph by beta ----

toronto_top_topics %>%
  # so we can visualize overall frequency
  left_join(toronto_all_word_count, by = c("term" = "word")) %>% 
  ggplot() + 
  geom_col(aes(x = reorder(term, beta), y = beta, fill = topic), show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", nrow = 4) + 
  scale_fill_distiller(palette = "Dark2") +
  coord_flip() +
  hrbrthemes::theme_ipsum_rc(base_size = 12, grid = "X") +# theme(strip.text = element_text(colour = "white")) + 
  ggtitle("Putative Topics Discussed in r/Toronto - As Determined by LDA",
          "Comments from 2016 - 2018.")
# labs(y = "Beta (Probability of Word being Generated)", x = "", caption = "delvinso.github.io")

# ---- wordclouds ----


n <- 20
palette = "Dark2"
pal <- rep(brewer.pal(8, palette), each = ceiling(n / 8))[n:1]

# ---- individual word clouds ----
for(i in 1:length(unique(toronto_top_topics$topic))){
  
  # png(paste0("output/wordclouds/toronto_20/", i, "_Toronto.png"), 6 * 100, 6 * 100, res = 100)
  png(paste0("output/wordclouds/toronto_50/", i, "_Toronto.png"), 6 * 100, 6 * 100, res = 100)
  
  par(bg = "grey95")
  toronto_top_topics %>% 
    filter(topic == i) %>%
    arrange(desc(beta)) %>% 
    .[1:n,] %>% 
    with(.,
         wordcloud(term, 
                   freq = beta,
                   random.order = FALSE, rot.per = 0.05,
                   ordered.colors = TRUE, 
                   # colors = viridis::viridis(n = 15)[3:12])) # for n = 10
                  colors = viridis::viridis(n = 35)[10:29])) # for n = 20

  title(paste("/r/Toronto Topic", i))
  dev.off()
}
# create a gif of topics
system('magick -loop 0 -delay 80 output/wordclouds/toronto_20/*_Toronto.png "output/wordclouds/toronto_20/Toronto_Topics.gif"')
system('magick -loop 0 -delay 80 output/wordclouds/toronto_50/*_Toronto.png "output/wordclouds/toronto_50/Toronto_Topics.gif"')

# ---- ensemble word clouds ----
n <- 20
#palette = "Dark2"
# pal <- rep(brewer.pal(8, palette), each = ceiling(n / 9))[n:1]

par(mfrow = c(5, 5), xpd = NA) # for 25 topics
par(bg = "grey95")
par(mar = c(1, 1, 1 ,1)) # default is c(5, 4, 4, 2) + 0.1.

par(mfrow = c(4, 5), xpd = NA) # for 20 topics

# for(i in 1:25){ # for 50 topics
# for(i in 26:50){ # for 50 topics
for(i in 1:length(unique(toronto_top_topics$topic))){ # with 20 topics
  toronto_top_topics %>%
    filter(topic == i) %>%
    arrange(desc(beta)) %>% 
    # to scale down word size
    .[1:n,] %>% 
    with(., 
         wordcloud(term, 
                   freq = beta,
                   scale = c(2.5, 0.75),
                   random.order = FALSE,
                   # max.words = 30,
                   rot.per = 0.05,
                   ordered.colors = TRUE,
                   # colors = viridis::viridis(n = 15)[3:12]) # for n = 10
                  colors = viridis::viridis(n = 35)[10:29])
  
    )
  title(paste("/r/Toronto Topic", i))
  # dev.off()
}
par(mfrow = c(1,1))

# ---- ldavis ---
library(LDAvis)
# http://www.bnosac.be/index.php/blog/56-an-overview-of-text-mining-visualisations-possibilities-with-r-on-the-ceta-trade-agreement
json <- createJSON(phi = posterior(toronto_lda20)$terms,
                   theta = posterior(toronto_lda20)$topics,
                   doc.length = slam::row_sums(toronto_words_dtm),
                   vocab = colnames(toronto_words_dtm),
                   term.frequency = slam::col_sums(toronto_words_dtm))
serVis(json)



# ---- predicting, or labelling the content of a new post ----
# https://stackoverflow.com/questions/16396090/r-topic-modeling-lda-model-labeling-function
# the assigned topics to each post of our training set
train_topics <- topics(toronto_train_lda50) 
# train_topics

# use our trained model to predict topics for new posts
test_topics <- posterior(toronto_train_lda50, toronto_test_dtm)
# colnames(test_topics$topics) <- apply(terms(toronto_train_lda20, 5), 2, paste, collapse = ", ")

test_topic_names <- apply(terms(toronto_train_lda50, 5), 2, paste, collapse = ", ") %>%
  broom::tidy() %>%
  mutate(topic = as.integer(str_split_fixed(names, " ", n = 2)[, 2]))  %>%
  select(-names) %>%
  rename("top_5_terms" = x)
# the columns of .$topics are the topics, rows are posts and the values are the 
# posterior probabilities

test_topics_tbl <- apply(test_topics$topics, 1, which.max) %>%
  data.frame() %>%
  rownames_to_column() %>%
  as_tibble() %>%
  set_names(c("post_id", "topic")) %>%
  left_join(test_topic_names)

test_topics_tbl 
write_csv(test_topics_tbl, "data/test_topics_toronto_2018_10.csv")
# write_csv(test_topics_tbl, "data/test_topics_toronto_rand_50_2018_10.csv")

test_topics_tbl <- read_csv("data/test_topics_toronto_2018_10.csv")
# test_topics_tbl <- read_csv("data/test_topics_toronto_rand_50_2018_10.csv")

# how well did our model do?


test_topics_tbl %>% 
  left_join(toronto_posts %>% select(title, date, id), by = c("post_id" = "id")) %>% 
  print(n = 50)


# random 50 

# we can take 50 at random... use submission title as proxy...

set.seed(1234)
sample_posts <- test_topics_tbl[sample(nrow(test_topics_tbl), 50), ]
sample_posts$post_id

toronto_posts$title[toronto_posts$id %in% sample_posts$post_id]


# random 50 

toronto_posts$title[toronto_posts$id %in% test_topics_tbl$post_id]